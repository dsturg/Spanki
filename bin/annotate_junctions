#!/usr/bin/env python
# encoding: utf-8
"""
annotate_junctions

This takes a junction list as input, along with annotation and fasta
It outputs generic information about the junction, including gene assignments

~/programs/spanki/bin/annotate_junctions -jtab oklist.txt -f ~/data/indexes/dm3_NISTnoUextra.fa -g ~/data/annotation/dmel_BDGP5.39.67_ed.gtf
"""
from __future__ import division 

import re
import sys
import argparse
import pysam
import csv
import math
import os
import collections

from pyfasta import Fasta
from datetime import datetime, date

# Biopython, for revcomp
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC

# Custom modules to import:
import spanki.spanki_parse_utils as spanki_parse_utils
import spanki.spanki_utils as spanki_utils

class Junc:
	"""
	Base class for a junction
	Expects SAM input
	"""
	def __init__(self, chr, start, cigar, readseq, tags):
		self.chr = chr
		self.start = start
		self.cigar = cigar
		self.readseq = readseq
		## anchorsize is the genomic coordinate space (accounting for indels)
		## The aligned portion may be longer or shorter after indels
		left_anchorsize = 0
		left_read_anchor = ""
		left_genome_aligned = ""
		right_anchorsize = 0
		right_read_anchor = ""
		right_genome_aligned = ""
		gapsize = 0
		gap_seen = 0
		readpos = 0
		genomepos = self.start
		for i in self.cigar:
			if i[0] == 0:
				if gap_seen < 1:
					left_anchorsize += i[1]
					left_read_anchor += self.readseq[readpos:readpos + i[1]]
					left_genome_aligned += Seq(f[self.chr][genomepos:genomepos + i[1]], IUPAC.unambiguous_dna)
				else:
					right_anchorsize += i[1]
					right_read_anchor += self.readseq[readpos:readpos + i[1]]
					right_genome_aligned += Seq(f[self.chr][genomepos:genomepos + i[1]], IUPAC.unambiguous_dna)
				readpos += i[1]
				genomepos += i[1]
			elif i[0] == 1:
				'''
				If insertion, add to readpos
				'''
				readpos += i[1]
			elif i[0] == 2:
				'''
				If deletion, add to anchor size
				'''
				if gap_seen < 1:
					left_anchorsize += i[1]
				else:
					right_anchorsize += i[1]
				genomepos += i[1]
			elif i[0] == 3:
				gap_seen += 1
				gapsize += i[1]
			else:
				quit("Don't recognize cigar code")

		
		self.left_read_anchor = left_read_anchor
		self.right_read_anchor = right_read_anchor
		self.left_genome_aligned = left_genome_aligned
		self.right_genome_aligned = right_genome_aligned
		self.gapsize = gapsize

		self.left_genome_anchor = Seq(f[self.chr][self.start:self.start + left_anchorsize], IUPAC.unambiguous_dna)
		self.right_genome_anchor = Seq(f[self.chr][self.start + left_anchorsize + gapsize:self.start + left_anchorsize + gapsize + right_anchorsize], IUPAC.unambiguous_dna)
		
		junc_left_side = self.start + left_anchorsize + 1
		junc_right_side = self.start + left_anchorsize + gapsize
		self.junc_left_side = junc_left_side
		self.junc_right_side = junc_right_side

		self.donormotif = Seq(f[self.chr][junc_left_side - 1:junc_left_side + 1], IUPAC.unambiguous_dna)
		self.acceptormotif = Seq(f[self.chr][junc_right_side - 2:junc_right_side], IUPAC.unambiguous_dna)
		self.dastring = str(self.donormotif + '..' + self.acceptormotif)
		
		""" 
		Try to get strand from SAM file first
		If it can't do it, use donor/acceptor motifs
		If still no, use *
		"""
		keys = []
		values = []
		for tag in tags:
			keys.append(tag[0])
			values.append(tag[1])
		tagdict = dict(zip(keys, values))
		try:
			strand = tagdict['XS']
			self.strand = strand
		except:
			if (self.dastring == "GT..AG"):
				self.strand = "+"
			elif (self.dastring == "GC..AG"):
				self.strand = "+"
			elif (self.dastring == "AT..AC"):
				self.strand = "+"
			elif (self.dastring == "CT..AC"):
				self.strand = "-"
			elif (self.dastring == "CT..GC"):
				self.strand = "-"
			elif (self.dastring == "GT..AT"):
				self.strand = "-"
			else: 
				self.strand = "*"
		
		self.juncid = self.chr + ":" + str(junc_left_side) + "_" + str(junc_right_side) + ":" + self.strand

	def display(self):
		print self.juncid
		print self.dastring
		print self.cigar
		print "start", self.start
		print "L read anchor   ", self.left_read_anchor
		print "L genome aligned", self.left_genome_aligned
		print "R read anchor   ", self.right_read_anchor
		print "R genome aligned", self.right_genome_aligned
		print "READ            ", self.readseq

'''
spankijunc version

class Junctionid:
	"""
	Base class for a junction, from juncid
	Expects junction id
	"""
	def __init__(self, juncid):
		chr = juncid.split(':')[0]
		coords = juncid.split(':')[1]
		strand = juncid.split(':')[2]
		start = int(coords.split('_')[0]) - 1
		end = int(coords.split('_')[1])
		self.chr = chr
		self.start = start
		self.end = end
		self.strand = strand.strip()
		self.intronsize = end - start
		self.accid = str(coords.split('_')[0])
		self.donid = chr + ":" + str(end)
'''
class Junctionid:
	"""
	Base class for a junction, from juncid
	Expects junction id
	"""
	def __init__(self, juncid):
		chr = juncid.split(':')[0]
		coords = juncid.split(':')[1]
		strand = juncid.split(':')[2]
		start = int(coords.split('_')[0]) - 1
		end = int(coords.split('_')[1])
		self.chr = chr
		self.start = start
		self.end = end
		self.strand = strand.strip()
		self.intronsize = end - start
		#self.accid = str(coords.split('_')[0])
		if self.strand == "+":
			self.donid = chr + ":" + str(start)
			self.donor = start
			self.accid = chr + ":" + str(end)
			self.donor = start
			self.acceptor = end
		elif self.strand == "-":
			self.accid = chr + ":" + str(start)
			self.donid = chr + ":" + str(end)
			self.donor = end
			self.acceptor = start
		else:
			print "strand character is", self.strand
			quit("Don't recognize strand")
	def display(self):
		print self.intronsize
		print self.chr
		print self.donid
		#print self.dastring
		#print "start", self.start
		#print "L read anchor   ", self.left_read_anchor
		#print "L genome aligned", self.left_genome_aligned
		#print "R read anchor   ", self.right_read_anchor
		#print "R genome aligned", self.right_genome_aligned
		#print "READ            ", self.readseq



def find_nagstring(myseq):
	#IRT = collections.defaultdict(lambda : collections.defaultdict(dict))
	'''
	Find nag acceptors
	'''
	nagstring = ""
	#print myseq
	nagstarts = []
	nagseqs = []

	p = re.compile('[ACGT]AG')
	#nags = p.findall(str(myseq))
	for nag in p.finditer(str(myseq)):
		#print nag.start(), nag.group()
		nagstarts.append(str(nag.start()))
		nagseqs.append(nag.group())
		#nagstring = nagstring + "," + nag

	#p = re.compile("[a-z]")
	#for m in p.finditer('a1b2c3d4'):
    #print m.start(), m.group()
	if (nagseqs):
		nagstring = ','.join(nagseqs) + ";" + ','.join(nagstarts)
	else:
		nagstring = "-"
	return nagstring

def find_nagstarts(myseq):
	#IRT = collections.defaultdict(lambda : collections.defaultdict(dict))
	'''
	Find nag acceptors
	'''
	nagstring = ""
	#print myseq
	nagstarts = []
	nagseqs = []

	p = re.compile('[ACGT]AG')
	#nags = p.findall(str(myseq))
	for nag in p.finditer(str(myseq)):
		#print nag.start(), nag.group()
		nagstarts.append(nag.start())
		nagseqs.append(nag.group())
		#nagstring = nagstring + "," + nag

	#p = re.compile("[a-z]")
	#for m in p.finditer('a1b2c3d4'):
    #print m.start(), m.group()
	#if (nagseqs):
	#	nagstring = ','.join(nagseqs) + ";" + ','.join(nagstarts)
	#else:
	#	nagstring = "-"
	return nagstarts

def tab_to_dict(tabfile):
	"""
	Generic make a dict from a table
	Assumes first column has key
	and there are column headers
	"""
	mytab = {}
	lines = csv.reader(open(tabfile, 'rb'), delimiter='\t')
	linecount = 0
	for line in lines:
		if (linecount < 1):
			"""
			First line is column header - use as keys
			"""
			keys = line
		else: 
			values = line
			linedict = dict(zip(keys, values))
			id = str(values[0])
			id = id.strip()
			mytab[id] = linedict 
			#print "adding to ",linedict['juncid']
			#print linedict
		linecount += 1
	return mytab

def parse_aligns_detailed(samfile):
	"""
	Takes a sam file as input
	Summarizes coverage, entropy, etc
	v.2 uses the Junc class
	"""
	JTAB = collections.defaultdict(int)
	UNFILT_JTAB = collections.defaultdict(int)
	STAB = collections.defaultdict(lambda : collections.defaultdict(dict))
	NEWDTAB = collections.defaultdict(lambda : collections.defaultdict(dict))
	MMES = collections.defaultdict(lambda : collections.defaultdict(dict))
	zero_anchor_warnings = 0
	for alignedread in samfile:
		if (len(alignedread.cigar) > 1):
			# Note that alignedread.is_reverse does not work right to get strand
			#strand = alignedread.tags[1][1] 
			mytid = alignedread.tid
			chr = samfile.getrname(mytid)
			start = alignedread.pos
			offset = alignedread.pos
			cigar = alignedread.cigar
			readseq = alignedread.query

			subcigars = subdivideCigar(cigar)

			for cigar in subcigars:
				j1 = Junc(chr,start,cigar,readseq,alignedread.tags)
				'''
				For multi-gap cigars, you have to add to the start
				for the next one
				'''
				start += int(len(j1.left_genome_aligned))
				start += j1.gapsize

				juncid = j1.juncid

				
				'''
				Apply a restriction on anchor size
				to count the junction
				'''
				min_anchor = min(len(j1.left_read_anchor),len(j1.right_read_anchor))

				'''
				Also count unfiltered (no anchor cutoff)
				'''

				if min_anchor < 1:
					'''
					Warn when anchor size is zero
					For example, cigar 76M54354N
					These are errors, and shouldn't count toward coverage
					'''
					zero_anchor_warnings += 1
				else: 
					UNFILT_JTAB[juncid] += 1; # Total junction spanning reads


				if min_anchor >= 8:
				
					JTAB[juncid] += 1; # Total junction spanning reads
	
					'''
					For MMES
					'''
					hdistl = hamming_distance(j1.left_read_anchor,j1.left_genome_aligned)
					hdistr = hamming_distance(j1.right_read_anchor,j1.right_genome_aligned)
					matchl = len(j1.left_read_anchor) - hdistl
					matchr = len(j1.right_read_anchor) - hdistr
					minmatch = matchl
					if matchr < matchl: minmatch = matchr
					if (MMES[j1.juncid]):
						currentmax = MMES[j1.juncid]['MAXmmes']
					else:
						currentmax = 0
						MMES[j1.juncid]['MAXminanc'] = 0
					if currentmax > minmatch:
						pass
					else: MMES[j1.juncid]['MAXmmes'] = minmatch
					
					# Get smallest ancor of this alinment
					minlen = len(j1.left_read_anchor)
					if minlen > len(j1.right_read_anchor):
						minlen = len(j1.right_read_anchor)
					
					# If the smallest anchor is bigger than current
					# maximium, change the dict entry
					if MMES[j1.juncid]['MAXminanc'] < minlen:
						MMES[j1.juncid]['MAXminanc'] = minlen
					
					''' 
					Original start of read is used as offset
					even if it is a multi-join read
					'''
					if STAB[juncid][offset]:
						STAB[juncid][offset] = STAB[juncid][offset] + 1
					else: 
						STAB[juncid][offset] = 1
	
					mysplit = juncid.split(":")
					coords = mysplit[1]
					donor, accid = coords.split("_")
					donid = chr + ":" + donor
					#donid, accid = juncid.split("_")
					if NEWDTAB[donid][accid]:
						NEWDTAB[donid][accid] += 1
					else:
						NEWDTAB[donid][accid] = 1
				else:
					#print "anchor too small", min_anchor, juncid, alignedread.qname
					pass

	# Coverage, indexed by donor
	# NEWDTAB[donid][accid] 	
	# Coverage, indexed by offset (For entropy calc)
	#STAB[juncid][offset] (count)
	# Count total junction spanning reads
	#JTAB[juncid] (count)
	if zero_anchor_warnings > 0:
		print "[ WARNING ] ", zero_anchor_warnings, "alignments with zero anchor coverage excluded"
	
	return JTAB,UNFILT_JTAB,STAB,NEWDTAB,MMES

def crossTabulate(x, fh):
	D = collections.defaultdict(int)
	for y in x:
		D[y] += 1
	x = D.keys()
	x.sort()
	for x in D.keys():
		print >> fh, "\t", x, "\t", D[x]


def findEdgeTx(chr,pos,strand,samfile):
	transcripts = []
	iter = samfile.fetch( chr, pos, pos + 1)
	for x in iter:
		samfilestrand = x.tags[0][1]
		txid = str(x.qname)
		if (strand == samfilestrand): 
			transcripts.append(txid)
	return transcripts

def listToKeys(mylist):
	D = defaultdict(int)
	for x in mylist:
		D[x] += 1
	x = D.keys()
	x.sort()
	newid = ",".join(x)
	return newid
	
def txlistToGeneList(mylist,lookup):
	D = collections.defaultdict(lambda : collections.defaultdict(dict))
	for x in mylist:
		if lookup[x]['gene_id']:
			geneid = lookup[x]['gene_id']
		else:
			geneid = "No gene assigned for this transcript"
		D[geneid] = 1
	x = D.keys()
	x.sort()
	return x

def donor_acceptor_transition(NEWDTAB,myjuncs):
	"""
	Gets donor-acceptor transition probabilities 
	using junction coverage
	"""
	DATRANS = collections.defaultdict(lambda : collections.defaultdict(dict))
	covbyedge = collections.defaultdict(lambda : collections.defaultdict(dict))
	joinsbyedge = collections.defaultdict(lambda : collections.defaultdict(dict))
	Ks = NEWDTAB.keys()
	Ks.sort()
	'''
	First calculate coverage by edge (donor or acceptor)
	Remember to check if each join has been filtered out
	'''
	for x in Ks:
		#dcov = sum(NEWDTAB[x].values())
		#covbyedge[x] = dcov
		covbyedge[x] = 0
		joinsbyedge[x] = 0
		for y in NEWDTAB[x].keys():
			'''
			Get juncid
			'''
			juncid = str(x) + "_" + str(y)
			if (juncid + ":+") in myjuncs:
				juncid = juncid + ":+"
			elif (juncid + ":-") in myjuncs:
				juncid = juncid + ":-"
			else:
				pass
			''' 
			Check to see if it was filtered
			'''
			if juncid in myjuncs:
				covbyedge[x] += NEWDTAB[x][y]
				joinsbyedge[x] += 1
				#jcov = NEWDTAB[x][y]
				#print "Looking at ", x, "to", y, "with jcov", jcov
				if covbyedge[y]:
					covbyedge[y] += NEWDTAB[x][y]
					joinsbyedge[y] += 1
				else:
					covbyedge[y] = NEWDTAB[x][y]
					joinsbyedge[y] = 1
	'''
	Now instantiate hash of neighbor coverages keyed by juncid
	Also do DATRANS
	'''
	NEIGHBORCOV = collections.defaultdict(lambda : collections.defaultdict(dict))
	for x in Ks:
		for y in NEWDTAB[x].keys():
			juncid = str(x) + "_" + str(y)
			if (juncid + ":+") in myjuncs:
				juncid = juncid + ":+"
			elif (juncid + ":-") in myjuncs:
				juncid = juncid + ":-"
			else:
				pass

			if (juncid in myjuncs):
				#print "Looking at neighbor coverage"
				jcov = NEWDTAB[x][y]
				#print "covbyedge", covbyedge[x]
				try:
					trans = jcov/covbyedge[x]
				except ZeroDivisionError:
					trans = 0
				DATRANS[juncid]['datrans'] = "%.3f" % trans
				NEIGHBORCOV[juncid]['dncov'] = covbyedge[x] - jcov
				NEIGHBORCOV[juncid]['ancov'] = covbyedge[y] - jcov
				NEIGHBORCOV[juncid]['dnjoins'] = joinsbyedge[x]
				NEIGHBORCOV[juncid]['anjoins'] = joinsbyedge[y]
	return DATRANS, NEIGHBORCOV, joinsbyedge

	
def intron_readthrough(myjuncs,samfile):
	IRT = collections.defaultdict(lambda : collections.defaultdict(dict))
	overhang = 8
	'''
	Get read length from first aligment
	'''
	for alignedread in samfile:
		readlength = alignedread.rlen
		break
	for juncid in myjuncs:

		j1 = Junctionid(juncid)

		# Check left side
		lirt = 0
		rangestart = j1.start - readlength - overhang
		rangeend = j1.start - overhang
		if (rangestart < 0): rangestart = 0
		# Need this for cases where a junciton is near 
		# the end of a chromosome
		
		
		# Get all reads in the region where intron read-thru reads
		# could reside.
		# Count all reads that start in the correct range and have no gaps.
		reads = samfile.fetch( j1.chr, rangestart, rangeend )		
		for read in reads:
			if (rangeend > read.pos > rangestart): 
				cigar = read.cigar
				gaps = 0
				if len(cigar) > 1:
					for i in cigar:
						if i[0] == 3: gaps += 1
				if gaps < 1: 
					lirt += 1
		# Check right side
		rirt = 0
		rangestart = j1.end - readlength + overhang
		rangeend = j1.end - overhang
		if (rangestart < 0): rangestart = 0
		reads = samfile.fetch( j1.chr, rangestart, rangeend )
		for read in reads:
			if (rangeend > read.pos > rangestart): 
				cigar = read.cigar
				gaps = 0
				if len(cigar) > 1:
					for i in cigar:
						if i[0] == 3: gaps += 1
				if gaps < 1: 
					rirt += 1
		IRT[juncid]['lirt'] = lirt
		IRT[juncid]['rirt'] = rirt
		IRT[juncid]['irt'] = lirt + rirt
	return IRT


def alignment_entropies(STAB,JTAB):
	"""
	Prints entropies and coverages
	"""
	""" juncid cov offsets entropy """
	""" chr2L:101195_101248:+	4 	3 	1.50 """
	ENTROPIES = collections.defaultdict(lambda : collections.defaultdict(dict))
	for x in STAB.keys():
		# Try to calculate entropy
		totalsamreads = JTAB[x]
		jctn_entropy = 0
		offsets = 0
		for y in STAB[x].keys():\
			# For each offset
			p_term = STAB[x][y] / totalsamreads
			jctn_entropy += p_term * math.log(p_term,2)
			offsets += 1
			#leftmost.append[y]
		jctn_entropy = math.fabs(jctn_entropy)
		ENTROPIES[x]['offsets'] = offsets
		ENTROPIES[x]['entropy'] = "%.6f" % jctn_entropy
	return ENTROPIES
	

def hamming_distance(s1, s2):
	assert len(s1) == len(s2)
	s1 = s1.upper()
	s2 = s2.upper()
	return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))

def subdivideCigar(cigar):
	# Get gap indices
	gaps = []
	counter = 0
	for i in cigar:
		if i[0] == 3: gaps.append(counter)
		counter += 1	
	# Define borders of junction regions
	if gaps:
		"""
		IF there are no gaps(juncs)
		return epty set
		"""
		borders = [0]
		for i in range(len(gaps[:-1])):
			borders.append(gaps[i+1])
			borders.append(gaps[i] + 1)
		borders.append(len(cigar))
		
		# Define subcigar ranges
		subcigars = []
		myjuncs = zip(borders[0::2], borders[1::2])
		for junc in myjuncs:
			subcigars.append(cigar[junc[0]:junc[1]])
	else:
		subcigars = []
	return subcigars

def printDict(mydict,fout):
	mykeys = mydict.keys()
	print >> fout, "juncid\t", '\t'.join(map(str,mydict[mykeys[1]].keys()))
	for x in mydict.keys():
		vals = []
		for field in mydict[x]:
			vals.append(mydict[x][field])
		print >> fout, x, '\t', '\t'.join(map(str,vals))

def print_dict_ordered(mydict,fout,fieldorder):
	#mykeys = fieldorder
	mykeys = mydict.keys()
	mykeys.sort()
	headings = ['juncid']
	headings.extend(fieldorder)
	print >> fout, '\t'.join(headings)

	#print >> fout, "juncid\t", '\t'.join(fieldorder)
	for x in mykeys:
		vals = []
		for field in fieldorder:
			vals.append(mydict[x][field])
		# Fixing it so it doesn't add a space to the junction id
		results = [x]
		results.extend(map(str,vals))
		print >> fout, '\t'.join(results)
		#print >> fout, x, '\t', '\t'.join(map(str,vals))

def uniq(seq): 
	# Not order preserving 
	keys = {} 
	for e in seq: 
		keys[e] = 1 
	return keys.keys()

################################
################################
################################
#
#Below is all stufff I had
# separated in jtools_analyses
################################
################################
def getJoinGeneAssign(myjuncs,refjuncs,edgedict,samfile,lookup):
	'''
	For unannotated joins, make a gene assignment 
	according to which genes it overlaps
	'''
	GENEASSIGN = collections.defaultdict(lambda : collections.defaultdict(dict))
	ANNOSTATUS = collections.defaultdict(lambda : collections.defaultdict(dict))
	
	for juncid in myjuncs:
		gtfassign = "-"
		transcripts = []
		donassign = []
		accassign = []
		j1 = Junctionid(juncid)
		ancode = ""
		if j1.strand == '+':
			donid = j1.chr + ":" + str(j1.start+1)
			accid = j1.chr + ":" + str(j1.end)
			if juncid in refjuncs: 
				ancode = "an"
				if edgedict.has_key(donid):
					transcripts = []
					transcripts.append(edgedict[donid])
					#print transcripts
					donassign = txlistToGeneList(transcripts,lookup)
				else:
					print donid, ' not in hash, but join ', juncid, ' is'
				
				if edgedict.has_key(accid):
					transcripts = []
					transcripts.append(edgedict[accid])
					accassign = txlistToGeneList(transcripts,lookup)
				else:
					print accid, ' not in hash, but join ', juncid, ' is'
				
				gtfassignlist = []
				gtfassignlist.extend(donassign)
				#print gtfassignlist
				gtfassignlist.extend(accassign)
				#print gtfassignlist
				gtfassignlist.sort()
				#print gtfassignlist
				gtfassignlist = list(set(gtfassignlist))
				gtfassign = ",".join(gtfassignlist)
			else: 
				ancode = "un"
				if edgedict.has_key(donid): 
					ancode = ancode + ".ad"
					transcripts = []
					transcripts.append(edgedict[donid])
					donassign = txlistToGeneList(transcripts,lookup)
				else:
					transcripts = findEdgeTx(j1.chr,j1.start,j1.strand,samfile)
					donassign = txlistToGeneList(transcripts,lookup)
				if edgedict.has_key(accid): 
					ancode = ancode + ".aa"
					transcripts = []
					transcripts.append(edgedict[accid])
					accassign = txlistToGeneList(transcripts,lookup)
				else:
					transcripts = findEdgeTx(j1.chr,j1.end - 1,j1.strand,samfile)
					accassign = txlistToGeneList(transcripts,lookup)
		else:
			accid = j1.chr + ":" + str(j1.start + 1)
			donid = j1.chr + ":" + str(j1.end)
			if juncid in refjuncs: 
				ancode = "an"
				if edgedict.has_key(donid):
					transcripts = []
					transcripts.append(edgedict[donid])
					donassign = txlistToGeneList(transcripts,lookup)
				else:
					print donid, ' not in hash, but join ', juncid, ' is'
				
				if edgedict.has_key(accid):
					transcripts = []
					transcripts.append(edgedict[accid])
					accassign = txlistToGeneList(transcripts,lookup)
				else:
					print accid, ' not in hash, but join ', juncid, ' is'
				gtfassignlist = []
				gtfassignlist.extend(donassign)
				#print gtfassignlist
				gtfassignlist.extend(accassign)
				#print gtfassignlist
				gtfassignlist.sort()
				#print gtfassignlist
				gtfassignlist = list(set(gtfassignlist))
				gtfassign = ",".join(gtfassignlist)
			else: 
				ancode = "un"
				if edgedict.has_key(donid): 
					ancode = ancode + ".ad"
					transcripts = []
					transcripts.append(edgedict[donid])
					donassign = txlistToGeneList(transcripts,lookup)
				else:
					transcripts = findEdgeTx(j1.chr,j1.end - 1,j1.strand,samfile)
					donassign = txlistToGeneList(transcripts,lookup)				
				if edgedict.has_key(accid): 
					ancode = ancode + ".aa"
					transcripts = []
					transcripts.append(edgedict[accid])
					accassign = txlistToGeneList(transcripts,lookup)
				else:
					transcripts = findEdgeTx(j1.chr,j1.start,j1.strand,samfile)
					accassign = txlistToGeneList(transcripts,lookup)
	
		
		goverlap = [gene for gene in donassign if gene in accassign]
		
		if goverlap:
			if len(goverlap) == 1:
				geneassign = goverlap[0]
			else:
				geneassign = "ambiguous"
		else:
			if donassign or accassign:
				''' 
				If either don or acc is assigned 
				'''
				geneassign = "ambiguous"
			else:
				geneassign = "none"

		if donassign: 
			donstring = ",".join(donassign)
		else:
			donstring = "none"
		if accassign: 
			accstring = ",".join(accassign)
		else:
			accstring = "none"
		
		ANNOSTATUS[juncid] = ancode
		GENEASSIGN[juncid]['geneassignAn'] = gtfassign
		GENEASSIGN[juncid]['geneassignL'] = donstring
		GENEASSIGN[juncid]['geneassignR'] = accstring
		GENEASSIGN[juncid]['geneassignOv'] = geneassign
		if (ancode == "an"):
			GENEASSIGN[juncid]['geneassign'] = gtfassign
		else:
			GENEASSIGN[juncid]['geneassign'] = geneassign		
	#samfile.close()
	return GENEASSIGN, ANNOSTATUS

def makeRefList(myjuncs,f):
 	"""
 	Gets the anchor seq of each annotated join for comparison
 	"""
 	mylist = []
	for juncid in myjuncs:
		j1 = Junctionid(juncid)
		if j1.strand == '+':
			fiveprimeflank = Seq(f[j1.chr][j1.start-10:j1.start], IUPAC.unambiguous_dna)
			threeprimeflank = Seq(f[j1.chr][j1.end:j1.end+10], IUPAC.unambiguous_dna)
			donormotif = Seq(f[j1.chr][j1.start:j1.start+2], IUPAC.unambiguous_dna)
			acceptormotif = Seq(f[j1.chr][j1.end-2:j1.end], IUPAC.unambiguous_dna)
		else:
			fiveprimeflank = Seq(f[j1.chr][j1.end:j1.end+10], IUPAC.unambiguous_dna)
			threeprimeflank = Seq(f[j1.chr][j1.start-10:j1.start], IUPAC.unambiguous_dna)
			fiveprimeflank = fiveprimeflank.reverse_complement()
			threeprimeflank = threeprimeflank.reverse_complement()
			acceptormotif = Seq(f[j1.chr][j1.start:j1.start+2], IUPAC.unambiguous_dna)
			donormotif = Seq(f[j1.chr][j1.end-2:j1.end], IUPAC.unambiguous_dna)
			dastring = donormotif.reverse_complement() + '..' + acceptormotif.reverse_complement()
		#print fiveprimeflank + ".." + threeprimeflank
		#print dastring, strand
		mylist.append(fiveprimeflank + threeprimeflank)
	return(mylist)

def intron_sequence(myjuncs,f,offset):
 	"""
 	Returns the intron sequence and flanks for each join
 	"""
 	INTSEQ = collections.defaultdict(lambda : collections.defaultdict(dict))
	for juncid in myjuncs:
		j1 = Junctionid(juncid)
		if j1.strand == '+':
			fiveprimeflank = Seq(f[j1.chr][j1.start-offset:j1.start], IUPAC.unambiguous_dna)
			threeprimeflank = Seq(f[j1.chr][j1.end:j1.end+offset], IUPAC.unambiguous_dna)
			fiveprimeIntron = Seq(f[j1.chr][j1.start:j1.start + offset], IUPAC.unambiguous_dna)
			threeprimeIntron = Seq(f[j1.chr][j1.end-offset:j1.end], IUPAC.unambiguous_dna)
			donormotif = Seq(f[j1.chr][j1.start:j1.start+2], IUPAC.unambiguous_dna)
			acceptormotif = Seq(f[j1.chr][j1.end-2:j1.end], IUPAC.unambiguous_dna)
			acceptormotif = acceptormotif.upper()
			donormotif = donormotif.upper()
			dastring = donormotif + '..' + acceptormotif
		else:
			fiveprimeflank = Seq(f[j1.chr][j1.end:j1.end+offset], IUPAC.unambiguous_dna)
			threeprimeflank = Seq(f[j1.chr][j1.start-offset:j1.start], IUPAC.unambiguous_dna)
			fiveprimeflank = fiveprimeflank.reverse_complement()
			threeprimeflank = threeprimeflank.reverse_complement()
			fiveprimeIntron = Seq(f[j1.chr][j1.end-offset:j1.end], IUPAC.unambiguous_dna)
			threeprimeIntron = Seq(f[j1.chr][j1.start:j1.start+offset], IUPAC.unambiguous_dna)
			fiveprimeIntron = fiveprimeIntron.reverse_complement()
			threeprimeIntron = threeprimeIntron.reverse_complement()
			acceptormotif = Seq(f[j1.chr][j1.start:j1.start+2], IUPAC.unambiguous_dna)
			donormotif = Seq(f[j1.chr][j1.end-2:j1.end], IUPAC.unambiguous_dna)
			acceptormotif = acceptormotif.upper()
			donormotif = donormotif.upper()
			dastring = donormotif.reverse_complement() + '..' + acceptormotif.reverse_complement()
		INTSEQ[juncid]['dinucleotide'] = dastring
		INTSEQ[juncid]['flank5'] = fiveprimeflank
		INTSEQ[juncid]['flank3'] = threeprimeflank
		INTSEQ[juncid]['intron5'] = fiveprimeIntron
		INTSEQ[juncid]['intron3'] = threeprimeIntron
	return INTSEQ
	
def anchor_hamming(myjuncs,f):
 	"""
 	Gets the hamming distances of 
 	interior intron sequence and 
 	exon anchor sequence
 	"""
 	ANCHAM = collections.defaultdict(lambda : collections.defaultdict(dict))
	for juncid in myjuncs:
		j1 = Junctionid(juncid)
		if j1.strand == '+':
			fiveprimeflank = Seq(f[j1.chr][j1.start-10:j1.start], IUPAC.unambiguous_dna)
			threeprimeflank = Seq(f[j1.chr][j1.end:j1.end+10], IUPAC.unambiguous_dna)
			donormotif = Seq(f[j1.chr][j1.start:j1.start+10], IUPAC.unambiguous_dna)
			acceptormotif = Seq(f[j1.chr][j1.end-10:j1.end], IUPAC.unambiguous_dna)
		else:
			acceptormotif = Seq(f[j1.chr][j1.start:j1.start+10], IUPAC.unambiguous_dna)
			donormotif = Seq(f[j1.chr][j1.end-10:j1.end], IUPAC.unambiguous_dna)
			fiveprimeflank = Seq(f[j1.chr][j1.end:j1.end+10], IUPAC.unambiguous_dna)
			threeprimeflank = Seq(f[j1.chr][j1.start-10:j1.start], IUPAC.unambiguous_dna)
			acceptormotif = acceptormotif.reverse_complement()
			donormotif = donormotif.reverse_complement()
			fiveprimeflank = fiveprimeflank.reverse_complement()
			threeprimeflank = threeprimeflank.reverse_complement()
		ANCHAM[juncid]['hamming5'] = hamming_distance(fiveprimeflank,acceptormotif)
		ANCHAM[juncid]['hamming3'] = hamming_distance(threeprimeflank,acceptormotif)
	return ANCHAM
	
def getGeneAnnotationCodes(samfile,lookup):
	'''
	Create a dict of features associated with genes
	And make a standardized code for each gene model
	'''
	GENES = collections.defaultdict(lambda : collections.defaultdict(dict))
	for alignedread in samfile:
		txid = str(alignedread.qname)
		geneid = lookup[txid]['gene_id']
		if geneid:
			if geneid in GENES.keys():		
				GENES[geneid]['tx'].append(txid)
				GENES[geneid]['start'].append(alignedread.pos)
			else:
				GENES[geneid]['tx'] = []
				GENES[geneid]['tx'].append(txid)
				GENES[geneid]['start'] = []
				GENES[geneid]['start'].append(alignedread.pos)
				GENES[geneid]['intron'] = []		
			if (len(alignedread.cigar) > 1):
				strand = alignedread.tags[0][1] 
				mytid = alignedread.tid
				txid = str(alignedread.qname)
				chr = samfile.getrname(mytid)
				start = alignedread.pos
				offset = start
				gaps = []
				matches = []
				addnextmatch = 0
				addtomatch = 0
				for i in alignedread.cigar:
					# Iterate over each entry in cigar
					# matches should be appended 
					if (i[0] == 0):
						if addnextmatch > 0:
							matches[-1] = matches[-1] + i[1] + addtomatch
							addnextmatch = 0
							addtomatch = 0
						else: matches.append(i[1])
					elif (i[0] == 3):
						gaps.append(i[1])
					elif (i[0] == 1):
						addnextmatch += 1
						addtomatch = 0
					elif (i[0] == 2):
						addnextmatch += 1
						addtomatch = i[1]
										
				if len(gaps) > 0:
					for i in range(len(gaps)):
						junc_left_side = start + matches[i] + 1
						junc_right_side = start + matches[i] + gaps[i] 
						juncid = str(chr) + ":" + str(junc_left_side) + "_" + str(junc_right_side) + ":" + str(strand)
						#if (strand == '+'):
						#	donid = chr + ":" + str(junc_left_side)
						#	accid = chr + ":" + str(junc_right_side)
						#else:
						#	donid = chr + ":" + str(junc_right_side)
						#	accid = chr + ":" + str(junc_left_side)
						GENES[geneid]['intron'].append(juncid)
						start = start + matches[i] + gaps[i];
	#samfile.close()
	for gene in GENES.keys():
		code = ""
		code += str(len(uniq(GENES[gene]['tx'])))
		code += "t."
		code += str(len(uniq(GENES[gene]['intron'])))
		code += "i."
		code += str(len(uniq(GENES[gene]['start'])))
		code += "s"
		regcode = ""
		if len(uniq(GENES[gene]['start'])) >  1:  
			regcode = "ap"
		if (len(uniq(GENES[gene]['tx'])) > len(uniq(GENES[gene]['start']))) and len(uniq(GENES[gene]['intron'])) > 0:  
			if regcode:
				regcode = "asap"
			else:
				regcode = "as"
		if len(uniq(GENES[gene]['intron'])) < 1 and len(uniq(GENES[gene]['start'])) < 2 :
		# NO introns, and 1 tx
			if regcode:
				#print regcode
				#print gene
				quit("already assigned")
			regcode = "se"
		if len(uniq(GENES[gene]['intron'])) > 0 and len(uniq(GENES[gene]['tx'])) < 2 :
		# >= 1 intron, but one transcript
			if regcode: 
				print regcode
				print gene
				quit("Already assigned")
			regcode = "con"
		
		if not regcode:
			print regcode
			print gene
			print GENES[gene]
			quit("No assignment!")
		# ap = multiple promoters
		# as = alternatively spliced (vs spliced)
		# asap = alt prom, alt spliced
		# con = spliced, but not alternatively
		# se = Not spliced, and no alt. promoters
		GENES[gene]['gmcode'] = code
		#print "gmcode: ", gmcode
		GENES[gene]['regcode'] = regcode
		#print "exoncode: ", regcode
	return GENES


def annotation_status(myjuncs,refjuncs,edgedict):
	ANNSTATUS = collections.defaultdict(lambda : collections.defaultdict(dict))
 	stati = []
	for juncid in myjuncs:
		j1 = Junctionid(juncid)
		if j1.strand == '+':
			if juncid in refjuncs: ancode = "an"
			else: 
				ancode = "un"
				donid = j1.chr + ":" + str(j1.start)
				accid = j1.chr + ":" + str(j1.end)
				if edgedict.has_key(donid): ancode = ancode + ".ad"
				if edgedict.has_key(accid): ancode = ancode + ".aa"
			#print >> gff_juncs_annot_out, juncid + '\t' + j1.chr + '\t' + j1.strand + '\t', j1.start, '\t', j1.end, '\t' + ancode 
			stati.appj1.end(ancode)
		else:
			if juncid in refjuncs: ancode = "an"
			else: 
				ancode = "un"
				accid = j1.chr + ":" + str(j1.start)
				donid = j1.chr + ":" + str(j1.end)
				if edgedict.has_key(donid): ancode = ancode + ".ad"
				if edgedict.has_key(accid): ancode = ancode + ".aa"
			#print >> gff_juncs_annot_out, juncid + '\t' +  j1.chr + '\t' + j1.strand + '\t', j1.start, '\t', j1.end, '\t' + ancode
			stati.appj1.end(ancode)
		#RESULTS[juncid]['annostatus'] = ancode
		ANNSTATUS[juncid]['annostatus'] = ancode
	print "Here is the annotation status breakdown of the detected junctions:"
	crossTabulate(stati)
	return ANNSTATUS


################################
################################
################################
################################
################################
class MyParser(argparse.ArgumentParser):
    def error(self, message):
        sys.stderr.write('error: %s\n' % message)
        #sys.stderr.write(desc)
        self.print_help()
        sys.exit(2)

def parse_options(desc):
	parser=MyParser(description=desc, formatter_class=argparse.RawTextHelpFormatter)
	#parser.add_argument('-i', help='BAM file name', action="store", dest="i")
	parser.add_argument('-jlist', help='junctionlist', action="store", dest="jlist")
	parser.add_argument('-jtab', help='junctiontab', action="store", dest="jtab")
	parser.add_argument('-g', help='Reference GTF', action="store", dest="g")
	#parser.add_argument('-m', action="store", dest="m", default="all",
	#	help="What method to run:\n"
	#	"\t'eval'  - Evaluates alignments, does not calculate IRT\n"
	#	"\t'quant' - Quantifies coverage and IRT, but not entropy and MMES\n"
	#	"\t'all'   - Performs all analyses (default)")
	parser.add_argument('-f', action="store", dest="f",
		help="Fasta file\n"
		"Must have same chromosomes as BAM and GTF")
	parser.add_argument('-o', help="Output directory, default='junctions_out'", action="store", dest="o", default="./junctions_out/")
	parser.add_argument('-s', type=int, help='Flanking intron sequence (number of bases)', action="store", dest="s")
	args = parser.parse_args()
	# See here for argparse instructions:
	# http://docs.python.org/dev/library/argparse.html
	if len(sys.argv)==1:
		parser.print_help()
		sys.exit(1)
	return args

# Initialize parameters
desc = '''
-----------------------------------------------------------------
annotate_junctions - Return descriptive information about junctions

Takes a junction list, and outputs a jtab with junction results.

From a reference annotation in GTF format, it will generate 
gene assignments for each junction, and codes for each
gene that specifies the number of and type of transcripts.

NOTE:  Spankijunc requires a reference fasta.  The reference
fasta, GTF, and BAM file must have matching chromosomes. In
other words, an alignment in the BAM must reference a chromosome
that is in the reference fasta
-----------------------------------------------------------------
'''

args = parse_options(desc)

#bamfile = args.i
jlist = args.jlist
jtab = args.jtab
outfile = args.o
gtffile = args.g
#method = args.m
fastafile = args.f
flankseq = args.s
#filter = args.filter

# Checking that input files exist
'''
Does not need a BAM file

if not os.path.exists(bamfile):
	print >> sys.stderr, "\n******************************************"
	print >> sys.stderr, "** Error: Could not find input bam file **"
	print >> sys.stderr, "******************************************\n\n"	
	parser=MyParser(description=desc, formatter_class=argparse.RawTextHelpFormatter)
	parser.print_help()
	exit(1)
'''


## If a gtf annotation was provided, check that it is readable
if gtffile:
	if not os.path.exists(gtffile):
		print >> sys.stderr, "\n******************************************"
		print >> sys.stderr, "** Error: Could not find input gtf file **"
		print >> sys.stderr, "******************************************\n\n"	
		parser=MyParser(description=desc, formatter_class=argparse.RawTextHelpFormatter)
		parser.print_help()
		exit(1)
if not os.path.exists(fastafile):
	print >> sys.stderr, "\n******************************************"
	print >> sys.stderr, "** Error: Could not find input fasta file **"
	print >> sys.stderr, "******************************************\n\n"	
	parser=MyParser(description=desc, formatter_class=argparse.RawTextHelpFormatter)
	parser.print_help()
	exit(1)


# Prepare output directory
output_dir = outfile

spanki_utils.prepare_output_dir(output_dir)

# Prepare output file names
juncs_seq_name = output_dir + "/juncs.seq"
juncs_seq = open(juncs_seq_name, "w")
juncs_out_name = output_dir + "/juncs.list"
juncs_out = open(juncs_out_name, "w")
juncs_all_out_name = output_dir + "/juncs.all"
juncs_all_out = open(juncs_all_out_name, "w")

don_out_name = output_dir + "/donors.txt"
don_out = open(don_out_name, "w")
acc_out_name = output_dir + "/acceptors.txt"
acc_out = open(acc_out_name, "w")

if gtffile:
	ann_summary_out_name = output_dir + "/annotation_summary.txt"
	ann_summary = open(ann_summary_out_name, "w")
#donor_out_name = output_dir + "/donors.txt"
#donor_out = open(donor_out_name, "w")
log_out_name = output_dir + "/logs/log.txt"
log_out = open(log_out_name, "w")
	
def main():
	
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Checking that required files are present
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	if not fastafile:
		quit("Need to specify a fasta file")
	#if not gtffile:
	#	quit("Need to specify a gtf file")
		
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Checking what type of run to do
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	#validmethods = ["quant","eval","test","all"]
	#if method in validmethods:
	#	print >> sys.stderr, "[**   Setup   **] Running in '", method, "' mode"
	#else:
	#	quit("[!!] Invalid method, choose 'eval' or 'quant'")

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Writing parameters to log file
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	print >> log_out, "[%s] Run started" % (spanki_utils.timestamp())
	#print >> log_out, "Method option:", method
	print >> log_out, "Fastafile:", fastafile
	if gtffile:
		print >> log_out, "Reference GTF file:", gtffile
	#print >> log_out, "Alignments:", bamfile


	
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Intializing the reference
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# You need the gtf file, and the fasta file
 	
 	if gtffile:
 		lookup = spanki_utils.prep_ref(gtffile,fastafile,output_dir)
  	## Note that you now have a reference called ref.bam, and a lookup dict
	tmp_dir = output_dir + "/tmp/"
 	reffile = tmp_dir + "/ref.bam"

	################################
	################################
	#  Loading data
	################################
	################################
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	# Initializing results hash
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#global RESULTS
	#RESULTS = collections.defaultdict(lambda : collections.defaultdict(dict))
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	# Load a fasta file
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	global f
	# Opening fasta filehandle
	print >> sys.stderr, "[%s] Opening fasta file" % (spanki_utils.timestamp())
	f = Fasta(fastafile)
	fastachr = set(sorted(f.keys()))

 	'''
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Loading read alignments
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	# Load bamfiles
	print >> sys.stderr, "[%s] Reading alignments from %s" % (spanki_utils.timestamp(), bamfile)
	

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Parse the read alignments
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	# Parse the bam file
	## Get a table of junctions, table of donors etc.
	bamfh = pysam.Samfile( bamfile, "rb" )
	bamchr = set(bamfh.references)
	bam_not_fasta = bamchr.difference(fastachr)
	fasta_not_bam = fastachr.difference(bamchr)
	if len(bam_not_fasta) > 0:
		print >> sys.stderr, "[%s] ERROR: Chromosomes in bam file header missing from fasta reference:" % (spanki_utils.timestamp()) 
		print >> sys.stderr, ','.join(bam_not_fasta)
		quit()
	if len(fasta_not_bam) > 0:
		print >> sys.stderr, "[%s] Warning: Chromosomes in fasta ref that have no alignments in bam:" % (spanki_utils.timestamp()) 
		print >> sys.stderr, ','.join(fasta_not_bam)
		
	JTAB,UNFILT_JTAB,STAB,NEWDTAB,MMES = parse_aligns_detailed(bamfh)
	'''	

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Load an annotation, flattened as bam
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	if gtffile:
	
		print >> sys.stderr, "[%s] Prototyping in/outdegree" % (spanki_utils.timestamp())
		reffh = pysam.Samfile( reffile, "rb" )
		EDGconnections, donlist, acclist = spanki_parse_utils.edgeConnections(reffh)
		reffh.close()
		acc_indegree = collections.defaultdict(lambda : collections.defaultdict(dict))
		don_outdegree = collections.defaultdict(lambda : collections.defaultdict(dict))
		for edge in acclist:
			acc_indegree[edge] = len(list(set(EDGconnections[edge])))
		for edge in donlist:
			don_outdegree[edge] = len(list(set(EDGconnections[edge])))
	
	
		print >> sys.stderr, "[%s] Trying to load annotation as bam" % (spanki_utils.timestamp())
		reffh = pysam.Samfile( reffile, "rb" )
		edgedict, refjuncs = spanki_parse_utils.parseRefAsBam(reffh)
		reffh.close()
		print >> sys.stderr, "[%s] Done loading annotation as bam" % (spanki_utils.timestamp())
		reffh = pysam.Samfile( reffile, "rb" )
		TXDATA, JUNCTIONS, JUNCSTARTD, JUNCENDD = spanki_parse_utils.get_junc_positions(reffh)
		reffh.close()
		#for tx in TXDATA.keys():
		#	print tx, TXDATA[tx]
		#for x in JUNCTIONS.keys():
		#	for y in JUNCENDD[x]:
		#		print 1000 - y
		#	print x, JUNCTIONS[x], "foo", JUNCSTARTD[x], "bar", JUNCENDD[x]
		#quit()
	
	
	
	
	
	#~~~~~~~~~~~~~~~~~~~
	# Load reference (input jtab) junction list
	# If no jtab specified, use the GTF list
	#~~~~~~~~~~~~~~~~~~~
	if jtab:
		reflist = tab_to_dict(jtab)
		myjuncs = reflist.keys()
	elif jlist:
		lines = csv.reader(open(jlist, 'rb'), delimiter='\t')
		myjuncs = []
		for line in lines:
			myjuncs.extend(line)
	else:
		myjuncs = refjuncs
		print >> sys.stderr, "No input junctions, analyzing annotation file"
	print >> sys.stderr, len(myjuncs), "in junction list"



	################################
	################################
	#  Analysis 1:  NAG/NAG
	#  Does not require gene models 
	#  Requires fasta sequence
	################################
	################################

	print >> sys.stderr, "[%s] Analyzing NAGNAG motifs" % (spanki_utils.timestamp())

	offset = 9
	
	NAG = collections.defaultdict(lambda : collections.defaultdict(dict))

	
	for x in myjuncs:
		#print '*', x
		j1 = Junctionid(x)
		#j1.display()
		nag0junc = "-"
		nag3junc = "-"
		nag6junc = "-"
		if j1.strand == "+":
			tempseq = Seq(f[j1.chr][j1.acceptor:j1.acceptor + offset], IUPAC.unambiguous_dna)
			nagstartsup = find_nagstarts(tempseq)
			tempseq = Seq(f[j1.chr][j1.acceptor - offset - 3:j1.acceptor - 3], IUPAC.unambiguous_dna)
			nagstartsdowntemp = find_nagstarts(tempseq)
			nagstartsdown = []
			for start in nagstartsdowntemp:
				start = start - offset - 3
				nagstartsdown.append(start)
			nagstarts = []
			nagstarts.extend(nagstartsup)
			nagstarts.extend(nagstartsdown)
			
			'''
			Make a string of all nag starts
			'''
			if len(nagstarts) < 1:
				nagstring = "-"
			else:
				s = map(str, nagstarts) 
				nagstring = ",".join(s)
		
			'''
			Make a string of all nags in frame
			'''
			nagframestarts = []
			for start in nagstarts:
				if start % 3 == 0: nagframestarts.append(start)
			if len(nagframestarts) < 1:
				nagframestring = "-"
			else:
				s = map(str, nagframestarts) 
				nagframestring = ",".join(s)
				
			if 6 in nagstarts:
				nag6junc = j1.chr + ":" + str(j1.donor + 1) + "_" + str(j1.acceptor + 9) + ":" + j1.strand
			if 3 in nagstarts:
				nag3junc = j1.chr + ":" + str(j1.donor + 1) + "_" + str(j1.acceptor + 6) + ":" + j1.strand
			if 0 in nagstarts:
				nag0junc = j1.chr + ":" + str(j1.donor + 1) + "_" + str(j1.acceptor + 3) + ":" + j1.strand

		elif j1.strand == "-":

			tempseq = Seq(f[j1.chr][j1.acceptor:j1.acceptor - offset], IUPAC.unambiguous_dna)
			tempseq = tempseq.reverse_complement()
			nagstartsup = find_nagstarts(tempseq)
			tempseq = Seq(f[j1.chr][j1.acceptor + 3:j1.acceptor + offset + 3], IUPAC.unambiguous_dna)
			tempseq = tempseq.reverse_complement()
			nagstartsdowntemp = find_nagstarts(tempseq)
			nagstartsdown = []
			for start in nagstartsdowntemp:
				start = start - offset - 3
				nagstartsdown.append(start)
			nagstarts = []
			nagstarts.extend(nagstartsup)
			nagstarts.extend(nagstartsdown)
			
			'''
			Make a frame of all nag starts
			'''
			if len(nagstarts) < 1:
				nagstring = "-"
			else:
				s = map(str, nagstarts) 
				nagstring = ",".join(s)
		
			'''
			Make a string of all nags in frame
			'''
			nagframestarts = []
			for start in nagstarts:
				if start % 3 == 0: nagframestarts.append(start)
			if len(nagframestarts) < 1:
				nagframestring = "-"
			else:
				s = map(str, nagframestarts) 
				nagframestring = ",".join(s)

			'''
			tempseq = Seq(f[j1.chr][j1.donor-offset:j1.donor], IUPAC.unambiguous_dna)
			tempseq = tempseq.reverse_complement()
			nagstring = find_nagstring(tempseq)
			nagframestring = "foo"
			'''
		else:
			quit("Don't recognize strand")
		NAG[x]['nagstring'] = nagstring
		NAG[x]['nagframestring'] = nagframestring
		NAG[x]['NAG0'] = nag0junc
		NAG[x]['NAG3'] = nag3junc
		NAG[x]['NAG6'] = nag6junc
		NAG[x]['TOTALFRAMENAG'] = len(nagframestarts)
			#fiveprimeflank = fiveprimeflank.reverse_complement()



	'''
	fiveprimeflank = Seq(f[j1.chr][j1.start-offset:j1.start], IUPAC.unambiguous_dna)
	threeprimeflank = Seq(f[j1.chr][j1.end:j1.end+offset], IUPAC.unambiguous_dna)
	fiveprimeIntron = Seq(f[j1.chr][j1.start:j1.start + offset], IUPAC.unambiguous_dna)
	threeprimeIntron = Seq(f[j1.chr][j1.end-offset:j1.end], IUPAC.unambiguous_dna)
	donormotif = Seq(f[j1.chr][j1.start:j1.start+2], IUPAC.unambiguous_dna)
	acceptormotif = Seq(f[j1.chr][j1.end-2:j1.end], IUPAC.unambiguous_dna)
	acceptormotif = acceptormotif.upper()
	donormotif = donormotif.upper()
	dastring = donormotif + '..' + acceptormotif
	else:
	fiveprimeflank = Seq(f[j1.chr][j1.end:j1.end+offset], IUPAC.unambiguous_dna)
	threeprimeflank = Seq(f[j1.chr][j1.start-offset:j1.start], IUPAC.unambiguous_dna)
	fiveprimeflank = fiveprimeflank.reverse_complement()
	threeprimeflank = threeprimeflank.reverse_complement()
	fiveprimeIntron = Seq(f[j1.chr][j1.end-offset:j1.end], IUPAC.unambiguous_dna)
	threeprimeIntron = Seq(f[j1.chr][j1.start:j1.start+offset], IUPAC.unambiguous_dna)
	fiveprimeIntron = fiveprimeIntron.reverse_complement()
	threeprimeIntron = threeprimeIntron.reverse_complement()
	acceptormotif = Seq(f[j1.chr][j1.start:j1.start+2], IUPAC.unambiguous_dna)
	donormotif = Seq(f[j1.chr][j1.end-2:j1.end], IUPAC.unambiguous_dna)
	acceptormotif = acceptormotif.upper()
	donormotif = donormotif.upper()
	dastring = donormotif.reverse_complement() + '..' + acceptormotif.reverse_complement()
	'''



	myjuncs.sort()

	# Coverage, indexed by donor
	# NEWDTAB[donid][accid] 	
	# Coverage, indexed by offset (For entropy calc)
	#STAB[juncid][offset] (count)
	# Count total junction spanning reads
	#JTAB[juncid] (count)
	
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Print junction list to the output directory
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	#Ks = JTAB.keys()
	for junc in myjuncs:
		print >> juncs_out, junc


	################################
	################################
	#  Analysis 2:  Annotation assessment
	#  Requires gene models 
	################################
	################################

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Make gene model type definitions 
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	if gtffile:
		print >> sys.stderr, "[%s] Getting gene model annotation codes" % (spanki_utils.timestamp())
		reffh = pysam.Samfile( reffile, "rb" )
		GENES = getGeneAnnotationCodes(reffh,lookup)
		reffh.close()
		print >> sys.stderr, "[%s] Done getting gene model annotation codes" % (spanki_utils.timestamp())
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		# Try gene assignment of unannotated joins
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		print >> sys.stderr, "[%s] Getting gene assignments for unannotated joins" % (spanki_utils.timestamp())
		reffh = pysam.Samfile( reffile, "rb" )
		GENEASSIGN, ANNOSTATUS = getJoinGeneAssign(myjuncs,refjuncs,edgedict,reffh,lookup)
		reffh.close()
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		# Add gene-level gene-model annotation codes 
		#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		GENEINFO = collections.defaultdict(lambda : collections.defaultdict(dict))
		for juncid in myjuncs:
			geneid = GENEASSIGN[juncid]['geneassign']
			if geneid in ["ambiguous","none"]:
				GENEINFO[juncid]['gmcode'] = "-"
				GENEINFO[juncid]['regcode'] = "-"
			else:
				#print GENES[geneid]['gmcode']
				'''
				If more than one gene, take the first
				'''
				mygene = str.split(geneid,",")[0]
				geneid = mygene
				GENEINFO[juncid]['gmcode'] = GENES[geneid]['gmcode']
				GENEINFO[juncid]['regcode'] = GENES[geneid]['regcode']
		print >> sys.stderr, "[%s] Done adding gene-model codes to results hash" % (spanki_utils.timestamp())

	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Prototype models for sequence comparison
 	# of novel juncs to annotated juncs
 	# (Not implemented yet)
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	'''
	if (method == "test"):
		refseqs = makeRefList(refjuncs,f)
		# Testing iterative search
		allmind = dict(zip(range(0,20), [0] * 20))
		unjuncs = 0
		print >> sys.stderr, "[%s] starting seq comp of novel juncs %s" % (spanki_utils.timestamp(), output_dir)
		
		for juncid in myjuncs:
			if juncid not in refjuncs:
				#print juncid, "not in reference, checking..."
				unjuncs += 1
				mind = 20
				chr = juncid.split(':')[0]
				coords = juncid.split(':')[1]
				strand = juncid.split(':')[2]
				#print juncid, strand
				start = int(coords.split('_')[0]) - 1
				end = int(coords.split('_')[1])
				if strand == '+':
					fiveprimeflank = Seq(f[chr][start-10:start], IUPAC.unambiguous_dna)
					threeprimeflank = Seq(f[chr][end:end+10], IUPAC.unambiguous_dna)
				else:
					fiveprimeflank = Seq(f[chr][end:end+10], IUPAC.unambiguous_dna)
					threeprimeflank = Seq(f[chr][start-10:start], IUPAC.unambiguous_dna)
				for seq in refseqs:
					h1 = hamming_distance(seq,fiveprimeflank + threeprimeflank)
					h2 = hamming_distance(seq,fiveprimeflank.reverse_complement() + threeprimeflank.reverse_complement())
					if (h1 < mind): 
						mind = h1
					if (h2 < mind):
						mind = h2
				print juncid, mind
				allmind[mind] += 1
		
		print "There were ", unjuncs, "unannotated juncs"
		for i in allmind.keys():
			print i, allmind[i]

		print >> sys.stderr, "[%s] Done seq comp of novel juncs %s" % (spanki_utils.timestamp(), output_dir)
		quit()
	'''
	################################
	################################
	#  Analysis 3:  Sequence analysis
	#  Does not require gene models
	#  Requires fasta
	################################
	################################

 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Extract sequence info for each join
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	
 	#offset = 20
 	offset = flankseq
 	INTSEQ = intron_sequence(myjuncs,f,offset)
	print >> sys.stderr, "[%s] Extracted intron sequence" % (spanki_utils.timestamp())

	################################
	################################
	#  Analysis 4:  Hamming
	#  Does not require gene models 
	#  Requires fasta sequence
	################################
	################################
	print >> sys.stderr, "[%s] Getting intron / anchor similarity" % (spanki_utils.timestamp())
	ANCHAM = anchor_hamming(myjuncs,f)

	################################
	################################
	# Preparing output tables
	################################
	################################
	

	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Print overall summary table
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	# Automatically, without regard to field order:
 	# printDict(RESULTS,juncs_all_out)
	# or manually, to get the order you want
	
	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Summarizing annotation statuses
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	if gtffile:
		stati = []
		for juncid in ANNOSTATUS.keys():
			stati.append(ANNOSTATUS[juncid])
		print >> ann_summary, "Annotation status breakdown of the detected junctions:"
		crossTabulate(stati,ann_summary)
	
	myjuncs.sort()

	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Make junction results table
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	RESULTS = collections.defaultdict(lambda : collections.defaultdict(dict))

	if gtffile:
		fieldorder = ['annostatus','geneassignL','geneassignR','geneassignOv','geneassignAn','geneassign','gmcode','regcode','dinucleotide','intron_size','nag','nagframe','nag0','nag3','nag6','totalframenag','hamming3','hamming5']
		for juncid in myjuncs:
			j1 = Junctionid(juncid)
			RESULTS[juncid]['intron_size'] = j1.intronsize
			RESULTS[juncid]['annostatus'] = ANNOSTATUS[juncid]
			RESULTS[juncid]['geneassignL'] = GENEASSIGN[juncid]['geneassignL']
			RESULTS[juncid]['geneassignR'] = GENEASSIGN[juncid]['geneassignR']
			RESULTS[juncid]['geneassign'] = GENEASSIGN[juncid]['geneassign']
			RESULTS[juncid]['geneassignAn'] = GENEASSIGN[juncid]['geneassignAn']
			RESULTS[juncid]['geneassignOv'] = GENEASSIGN[juncid]['geneassignOv']
			RESULTS[juncid]['gmcode'] = GENEINFO[juncid]['gmcode']
			RESULTS[juncid]['regcode'] = GENEINFO[juncid]['regcode']
			RESULTS[juncid]['dinucleotide'] = INTSEQ[juncid]['dinucleotide']
			RESULTS[juncid]['nag'] = NAG[juncid]['nagstring']
			RESULTS[juncid]['nagframe'] = NAG[juncid]['nagframestring'] 
			RESULTS[juncid]['nag0'] = NAG[juncid]['NAG0'] 
			RESULTS[juncid]['nag3'] = NAG[juncid]['NAG3'] 
			RESULTS[juncid]['nag6'] = NAG[juncid]['NAG6']
			RESULTS[juncid]['totalframenag'] = NAG[juncid]['TOTALFRAMENAG'] 
			RESULTS[juncid]['hamming3'] =  ANCHAM[juncid]['hamming3']
			RESULTS[juncid]['hamming5'] =  ANCHAM[juncid]['hamming5']
	else:
		fieldorder = ['dinucleotide','intron_size','nag','nagframe','nag0','nag3','nag6','totalframenag','hamming3','hamming5']
		for juncid in myjuncs:
			j1 = Junctionid(juncid)
			RESULTS[juncid]['intron_size'] = j1.intronsize
			RESULTS[juncid]['dinucleotide'] = INTSEQ[juncid]['dinucleotide']
			RESULTS[juncid]['nag'] = NAG[juncid]['nagstring']
			RESULTS[juncid]['nagframe'] = NAG[juncid]['nagframestring'] 
			RESULTS[juncid]['nag0'] = NAG[juncid]['NAG0'] 
			RESULTS[juncid]['nag3'] = NAG[juncid]['NAG3'] 
			RESULTS[juncid]['nag6'] = NAG[juncid]['NAG6']
			RESULTS[juncid]['totalframenag'] = NAG[juncid]['TOTALFRAMENAG'] 
			RESULTS[juncid]['hamming3'] =  ANCHAM[juncid]['hamming3']
			RESULTS[juncid]['hamming5'] =  ANCHAM[juncid]['hamming5']
	
	

	print_dict_ordered(RESULTS,juncs_all_out,fieldorder)


	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Printing adjoining sequence
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

	fieldorder = ['flank5','intron5','intron3','flank3']
	#for juncid in myjuncs:
	#	INTSEQ[juncid]['flank5'] = fiveprimeflank
	#	INTSEQ[juncid]['flank3'] = threeprimeflank
	#	INTSEQ[juncid]['intron5'] = fiveprimeIntron
	#	INTSEQ[juncid]['intron3'] = threeprimeIntron

	print_dict_ordered(INTSEQ,juncs_seq,fieldorder)		
	#printDict(RESULTS,juncs_all_out)

	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	# Printing donors and acceptors
 	#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
 	
	if gtffile:
		for edge in donlist:
			print >> don_out, edge, don_outdegree[edge] 
		for edge in acclist:
			print >> acc_out, edge, acc_indegree[edge] 




	print >> log_out, "[%s] Run completed" % (spanki_utils.timestamp())
	
	

if __name__ == "__main__":
    sys.exit(main())

